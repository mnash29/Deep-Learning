{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, abspath, join, curdir\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import RMSprop\n",
    "from torch.autograd import Variable\n",
    "from torch import FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3883, 3), (6040, 5), (1000209, 4))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset\n",
    "datapath = join(dirname(dirname(abspath(curdir))), \"data\", \"raw\", \"rbm\")\n",
    "\n",
    "movies = pd.read_csv(join(datapath, \"movielens-1m\", \"movies.dat\"),\n",
    "                     sep=\"::\",\n",
    "                     header=None,\n",
    "                     engine=\"python\",\n",
    "                     encoding=\"latin-1\")\n",
    "\n",
    "users = pd.read_csv(join(datapath, \"movielens-1m\", \"users.dat\"),\n",
    "                    sep=\"::\",\n",
    "                    header=None,\n",
    "                    engine=\"python\",\n",
    "                    encoding=\"latin-1\")\n",
    "\n",
    "ratings = pd.read_csv(join(datapath, \"movielens-1m\", \"ratings.dat\"),\n",
    "                      sep=\"::\",\n",
    "                      header=None,\n",
    "                      engine=\"python\",\n",
    "                      encoding=\"latin-1\")\n",
    "\n",
    "movies.shape, users.shape, ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 4), (20000, 4))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare training and test sets\n",
    "train_df = pd.read_csv(join(datapath, \"movielens-100k\", \"u1.base\"),\n",
    "                        delimiter=\"\\t\",\n",
    "                        header=None)\n",
    "\n",
    "train_set = np.array(train_df, dtype=\"int\")\n",
    "\n",
    "test_df = pd.read_csv(join(datapath, \"movielens-100k\", \"u1.test\"),\n",
    "                        delimiter=\"\\t\",\n",
    "                        header=None)\n",
    "\n",
    "test_set = np.array(test_df, dtype=\"int\")\n",
    "\n",
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create matrices of total number of users and movies for bi-fold cross validation\n",
    "# The max user/movie ID may be present in the training or test data\n",
    "nb_users = int(max(max(train_set[:, 0]), max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(train_set[:, 1]), max(test_set[:, 1])))\n",
    "\n",
    "nb_users, nb_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data: np.ndarray) -> list:\n",
    "    \"\"\"Convert data into a matrix like structure.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    data : np.ndarray\n",
    "        The data to transform\n",
    "    size : int\n",
    "        The total number of items in the overall dataset\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        The data transformed\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    for user_id in range(1, nb_users + 1):\n",
    "        # Get user movies and ratings\n",
    "        movie_ids = data[:, 1][data[:, 0] == user_id]\n",
    "        rating_ids = data[:, 2][data[:, 0] == user_id]\n",
    "\n",
    "        # Get all list of movie ratings by user, unrated movies = 0\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[movie_ids - 1] = rating_ids # movie_ids starts at 1\n",
    "        new_data.append(list(ratings))\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_converted = convert(train_set)\n",
    "test_set_converted = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into tensors\n",
    "train_set_ft = FloatTensor(train_set_converted)\n",
    "test_set_ft = FloatTensor(test_set_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([943, 1682])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([943, 1682])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stacked AutoEncoder instance\n",
    "from ae import SAE\n",
    "\n",
    "sae = SAE(nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criteria for loss function\n",
    "criterion = MSELoss()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train_Loss: 1.771719217300415\n",
      "Epoch: 1, Train_Loss: 1.0967650413513184\n",
      "Epoch: 2, Train_Loss: 1.053282380104065\n",
      "Epoch: 3, Train_Loss: 1.0383638143539429\n",
      "Epoch: 4, Train_Loss: 1.0307718515396118\n",
      "Epoch: 5, Train_Loss: 1.026605248451233\n",
      "Epoch: 6, Train_Loss: 1.0237427949905396\n",
      "Epoch: 7, Train_Loss: 1.0219579935073853\n",
      "Epoch: 8, Train_Loss: 1.0207545757293701\n",
      "Epoch: 9, Train_Loss: 1.0195647478103638\n",
      "Epoch: 10, Train_Loss: 1.0189452171325684\n",
      "Epoch: 11, Train_Loss: 1.0184614658355713\n",
      "Epoch: 12, Train_Loss: 1.01790452003479\n",
      "Epoch: 13, Train_Loss: 1.017511248588562\n",
      "Epoch: 14, Train_Loss: 1.0172500610351562\n",
      "Epoch: 15, Train_Loss: 1.0167913436889648\n",
      "Epoch: 16, Train_Loss: 1.0165694952011108\n",
      "Epoch: 17, Train_Loss: 1.016481637954712\n",
      "Epoch: 18, Train_Loss: 1.0163096189498901\n",
      "Epoch: 19, Train_Loss: 1.0160201787948608\n",
      "Epoch: 20, Train_Loss: 1.0161055326461792\n",
      "Epoch: 21, Train_Loss: 1.0157771110534668\n",
      "Epoch: 22, Train_Loss: 1.0157653093338013\n",
      "Epoch: 23, Train_Loss: 1.0157520771026611\n",
      "Epoch: 24, Train_Loss: 1.0159246921539307\n",
      "Epoch: 25, Train_Loss: 1.0155760049819946\n",
      "Epoch: 26, Train_Loss: 1.01566743850708\n",
      "Epoch: 27, Train_Loss: 1.014766812324524\n",
      "Epoch: 28, Train_Loss: 1.012969732284546\n",
      "Epoch: 29, Train_Loss: 1.0112814903259277\n",
      "Epoch: 30, Train_Loss: 1.010520100593567\n",
      "Epoch: 31, Train_Loss: 1.007990837097168\n",
      "Epoch: 32, Train_Loss: 1.0074788331985474\n",
      "Epoch: 33, Train_Loss: 1.0032633543014526\n",
      "Epoch: 34, Train_Loss: 1.002302885055542\n",
      "Epoch: 35, Train_Loss: 1.001549482345581\n",
      "Epoch: 36, Train_Loss: 0.9993302822113037\n",
      "Epoch: 37, Train_Loss: 0.9956708550453186\n",
      "Epoch: 38, Train_Loss: 0.9954097270965576\n",
      "Epoch: 39, Train_Loss: 0.9937818050384521\n",
      "Epoch: 40, Train_Loss: 0.9942677617073059\n",
      "Epoch: 41, Train_Loss: 0.9912219047546387\n",
      "Epoch: 42, Train_Loss: 0.9892561435699463\n",
      "Epoch: 43, Train_Loss: 0.9862791895866394\n",
      "Epoch: 44, Train_Loss: 0.9879413843154907\n",
      "Epoch: 45, Train_Loss: 0.9815248847007751\n",
      "Epoch: 46, Train_Loss: 0.9836858510971069\n",
      "Epoch: 47, Train_Loss: 0.9793722033500671\n",
      "Epoch: 48, Train_Loss: 0.9777895212173462\n",
      "Epoch: 49, Train_Loss: 0.9728564620018005\n",
      "Epoch: 50, Train_Loss: 0.980319619178772\n",
      "Epoch: 51, Train_Loss: 0.9807820916175842\n",
      "Epoch: 52, Train_Loss: 0.9785158038139343\n",
      "Epoch: 53, Train_Loss: 0.9809918999671936\n",
      "Epoch: 54, Train_Loss: 0.9817742705345154\n",
      "Epoch: 55, Train_Loss: 0.9711477756500244\n",
      "Epoch: 56, Train_Loss: 0.976378321647644\n",
      "Epoch: 57, Train_Loss: 0.9740018844604492\n",
      "Epoch: 58, Train_Loss: 0.977027952671051\n",
      "Epoch: 59, Train_Loss: 0.9730015397071838\n",
      "Epoch: 60, Train_Loss: 0.9733085036277771\n",
      "Epoch: 61, Train_Loss: 0.971700131893158\n",
      "Epoch: 62, Train_Loss: 0.972536027431488\n",
      "Epoch: 63, Train_Loss: 0.9655706286430359\n",
      "Epoch: 64, Train_Loss: 0.9651302099227905\n",
      "Epoch: 65, Train_Loss: 0.9624559283256531\n",
      "Epoch: 66, Train_Loss: 0.9648885130882263\n",
      "Epoch: 67, Train_Loss: 0.9648535847663879\n",
      "Epoch: 68, Train_Loss: 0.9646688103675842\n",
      "Epoch: 69, Train_Loss: 0.9621588587760925\n",
      "Epoch: 70, Train_Loss: 0.9641046524047852\n",
      "Epoch: 71, Train_Loss: 0.9590270519256592\n",
      "Epoch: 72, Train_Loss: 0.9581508040428162\n",
      "Epoch: 73, Train_Loss: 0.9539417624473572\n",
      "Epoch: 74, Train_Loss: 0.9521945118904114\n",
      "Epoch: 75, Train_Loss: 0.95306795835495\n",
      "Epoch: 76, Train_Loss: 0.9529157280921936\n",
      "Epoch: 77, Train_Loss: 0.9503403306007385\n",
      "Epoch: 78, Train_Loss: 0.9510118365287781\n",
      "Epoch: 79, Train_Loss: 0.949309766292572\n",
      "Epoch: 80, Train_Loss: 0.9484670162200928\n",
      "Epoch: 81, Train_Loss: 0.9459413886070251\n",
      "Epoch: 82, Train_Loss: 0.946121871471405\n",
      "Epoch: 83, Train_Loss: 0.9450029730796814\n",
      "Epoch: 84, Train_Loss: 0.9449074268341064\n",
      "Epoch: 85, Train_Loss: 0.9434596300125122\n",
      "Epoch: 86, Train_Loss: 0.9438410997390747\n",
      "Epoch: 87, Train_Loss: 0.9425222873687744\n",
      "Epoch: 88, Train_Loss: 0.942950427532196\n",
      "Epoch: 89, Train_Loss: 0.9413931369781494\n",
      "Epoch: 90, Train_Loss: 0.9418140649795532\n",
      "Epoch: 91, Train_Loss: 0.9402640461921692\n",
      "Epoch: 92, Train_Loss: 0.9401837587356567\n",
      "Epoch: 93, Train_Loss: 0.9381940364837646\n",
      "Epoch: 94, Train_Loss: 0.9393171668052673\n",
      "Epoch: 95, Train_Loss: 0.9396465420722961\n",
      "Epoch: 96, Train_Loss: 0.9390843510627747\n",
      "Epoch: 97, Train_Loss: 0.9372861385345459\n",
      "Epoch: 98, Train_Loss: 0.9373008608818054\n",
      "Epoch: 99, Train_Loss: 0.9361820220947266\n",
      "Epoch: 100, Train_Loss: 0.9369613528251648\n",
      "Epoch: 101, Train_Loss: 0.9355838298797607\n",
      "Epoch: 102, Train_Loss: 0.936014711856842\n",
      "Epoch: 103, Train_Loss: 0.9350171089172363\n",
      "Epoch: 104, Train_Loss: 0.9349249005317688\n",
      "Epoch: 105, Train_Loss: 0.9340413212776184\n",
      "Epoch: 106, Train_Loss: 0.934047520160675\n",
      "Epoch: 107, Train_Loss: 0.9335302114486694\n",
      "Epoch: 108, Train_Loss: 0.9333353638648987\n",
      "Epoch: 109, Train_Loss: 0.9333201050758362\n",
      "Epoch: 110, Train_Loss: 0.9328346848487854\n",
      "Epoch: 111, Train_Loss: 0.9329565763473511\n",
      "Epoch: 112, Train_Loss: 0.9328422546386719\n",
      "Epoch: 113, Train_Loss: 0.9328439831733704\n",
      "Epoch: 114, Train_Loss: 0.931435763835907\n",
      "Epoch: 115, Train_Loss: 0.9317627549171448\n",
      "Epoch: 116, Train_Loss: 0.9312053918838501\n",
      "Epoch: 117, Train_Loss: 0.9309152960777283\n",
      "Epoch: 118, Train_Loss: 0.93040931224823\n",
      "Epoch: 119, Train_Loss: 0.9304095506668091\n",
      "Epoch: 120, Train_Loss: 0.9295639991760254\n",
      "Epoch: 121, Train_Loss: 0.9297522306442261\n",
      "Epoch: 122, Train_Loss: 0.9290599822998047\n",
      "Epoch: 123, Train_Loss: 0.9290688633918762\n",
      "Epoch: 124, Train_Loss: 0.9280295372009277\n",
      "Epoch: 125, Train_Loss: 0.9284563064575195\n",
      "Epoch: 126, Train_Loss: 0.9275571703910828\n",
      "Epoch: 127, Train_Loss: 0.9280290603637695\n",
      "Epoch: 128, Train_Loss: 0.9275588393211365\n",
      "Epoch: 129, Train_Loss: 0.9279497265815735\n",
      "Epoch: 130, Train_Loss: 0.9267786741256714\n",
      "Epoch: 131, Train_Loss: 0.9270346760749817\n",
      "Epoch: 132, Train_Loss: 0.9264655113220215\n",
      "Epoch: 133, Train_Loss: 0.9264379739761353\n",
      "Epoch: 134, Train_Loss: 0.9258832335472107\n",
      "Epoch: 135, Train_Loss: 0.9257727861404419\n",
      "Epoch: 136, Train_Loss: 0.9250852465629578\n",
      "Epoch: 137, Train_Loss: 0.9253231287002563\n",
      "Epoch: 138, Train_Loss: 0.9253359436988831\n",
      "Epoch: 139, Train_Loss: 0.9251309037208557\n",
      "Epoch: 140, Train_Loss: 0.9244512319564819\n",
      "Epoch: 141, Train_Loss: 0.924312174320221\n",
      "Epoch: 142, Train_Loss: 0.9245977997779846\n",
      "Epoch: 143, Train_Loss: 0.9241641163825989\n",
      "Epoch: 144, Train_Loss: 0.9235415458679199\n",
      "Epoch: 145, Train_Loss: 0.9235742688179016\n",
      "Epoch: 146, Train_Loss: 0.9277132749557495\n",
      "Epoch: 147, Train_Loss: 0.923154890537262\n",
      "Epoch: 148, Train_Loss: 0.9228309392929077\n",
      "Epoch: 149, Train_Loss: 0.9225428700447083\n",
      "Epoch: 150, Train_Loss: 0.9216307401657104\n",
      "Epoch: 151, Train_Loss: 0.9214543104171753\n",
      "Epoch: 152, Train_Loss: 0.9217835068702698\n",
      "Epoch: 153, Train_Loss: 0.9218370914459229\n",
      "Epoch: 154, Train_Loss: 0.9215843677520752\n",
      "Epoch: 155, Train_Loss: 0.9215298295021057\n",
      "Epoch: 156, Train_Loss: 0.9211863279342651\n",
      "Epoch: 157, Train_Loss: 0.9213029742240906\n",
      "Epoch: 158, Train_Loss: 0.9213927388191223\n",
      "Epoch: 159, Train_Loss: 0.9206783771514893\n",
      "Epoch: 160, Train_Loss: 0.9205629825592041\n",
      "Epoch: 161, Train_Loss: 0.9203031659126282\n",
      "Epoch: 162, Train_Loss: 0.9201509952545166\n",
      "Epoch: 163, Train_Loss: 0.9199540019035339\n",
      "Epoch: 164, Train_Loss: 0.9196269512176514\n",
      "Epoch: 165, Train_Loss: 0.9193108081817627\n",
      "Epoch: 166, Train_Loss: 0.9197788834571838\n",
      "Epoch: 167, Train_Loss: 0.9195652604103088\n",
      "Epoch: 168, Train_Loss: 0.9191405177116394\n",
      "Epoch: 169, Train_Loss: 0.9191515445709229\n",
      "Epoch: 170, Train_Loss: 0.9189357161521912\n",
      "Epoch: 171, Train_Loss: 0.9185863733291626\n",
      "Epoch: 172, Train_Loss: 0.9186807870864868\n",
      "Epoch: 173, Train_Loss: 0.9181239008903503\n",
      "Epoch: 174, Train_Loss: 0.9180058836936951\n",
      "Epoch: 175, Train_Loss: 0.9180193543434143\n",
      "Epoch: 176, Train_Loss: 0.9183319211006165\n",
      "Epoch: 177, Train_Loss: 0.9178105592727661\n",
      "Epoch: 178, Train_Loss: 0.9177147746086121\n",
      "Epoch: 179, Train_Loss: 0.9175106287002563\n",
      "Epoch: 180, Train_Loss: 0.9172080159187317\n",
      "Epoch: 181, Train_Loss: 0.917191207408905\n",
      "Epoch: 182, Train_Loss: 0.9175164103507996\n",
      "Epoch: 183, Train_Loss: 0.9168658256530762\n",
      "Epoch: 184, Train_Loss: 0.9169174432754517\n",
      "Epoch: 185, Train_Loss: 0.9169326424598694\n",
      "Epoch: 186, Train_Loss: 0.9169706702232361\n",
      "Epoch: 187, Train_Loss: 0.9169048070907593\n",
      "Epoch: 188, Train_Loss: 0.9167128205299377\n",
      "Epoch: 189, Train_Loss: 0.916058361530304\n",
      "Epoch: 190, Train_Loss: 0.9156750440597534\n",
      "Epoch: 191, Train_Loss: 0.9157903790473938\n",
      "Epoch: 192, Train_Loss: 0.9154230952262878\n",
      "Epoch: 193, Train_Loss: 0.9154093861579895\n",
      "Epoch: 194, Train_Loss: 0.9150714874267578\n",
      "Epoch: 195, Train_Loss: 0.9152414798736572\n",
      "Epoch: 196, Train_Loss: 0.915300190448761\n",
      "Epoch: 197, Train_Loss: 0.9151054620742798\n",
      "Epoch: 198, Train_Loss: 0.9145004153251648\n",
      "Epoch: 199, Train_Loss: 0.9145373702049255\n"
     ]
    }
   ],
   "source": [
    "# Train the SAE model\n",
    "epochs = 200\n",
    "bias = 1e-10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "\n",
    "    for uid in range(nb_users):\n",
    "        # Get user ratings\n",
    "        input_vect = Variable(train_set_ft[uid]).unsqueeze(0)\n",
    "        target_vect = input_vect.clone()\n",
    "\n",
    "        # Skip any user with zero ratings\n",
    "        if torch.sum(target_vect.data > 0) > 0:\n",
    "            # Get vector of predicted ratings\n",
    "            output_vect = sae(input_vect)\n",
    "\n",
    "            # Don't compute gradient with respect to the targets (memory saver)\n",
    "            target_vect.requires_grad = False\n",
    "\n",
    "            # Freeze features where there was no rating by the user\n",
    "            output_vect[target_vect == 0] = 0\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output_vect, target_vect)\n",
    "\n",
    "            # Compute average of error where there was a rating\n",
    "            mean_corrector = nb_movies/float(torch.sum(target_vect.data > 0) + bias)\n",
    "\n",
    "            # Get direction of gradient to update weights\n",
    "            loss.backward()\n",
    "\n",
    "            # Compute RMSE and update train_loss\n",
    "            train_loss += np.sqrt(loss.data * mean_corrector)\n",
    "            s += 1.\n",
    "\n",
    "            # Get intensity of weight updates\n",
    "            optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Train_Loss: {train_loss/s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Loss: 0.9517191052436829\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test set\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "for uid in range(nb_users):\n",
    "    input_vect = Variable(train_set_ft[uid]).unsqueeze(0)\n",
    "    target_vect = Variable(test_set_ft[uid]).unsqueeze(0)\n",
    "\n",
    "    if torch.sum(target_vect.data > 0) > 0:\n",
    "        output_vect = sae(input_vect)\n",
    "        target_vect.requires_grad = False\n",
    "        output_vect[target_vect == 0] = 0\n",
    "        loss = criterion(output_vect, target_vect)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target_vect.data > 0) + bias)\n",
    "        test_loss += np.sqrt(loss.data * mean_corrector)\n",
    "        s += 1.\n",
    "\n",
    "print(f\"Test_Loss: {test_loss/s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
