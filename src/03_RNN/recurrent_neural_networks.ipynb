{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Ultimate Guide to Recurrent Neural Networks](https://www.superdatascience.com/blogs/the-ultimate-guide-to-recurrent-neural-networks-rnn)\n",
    "* Used for `Time Series` data analysis\n",
    "* Weights like in ANNs represent long-term memory\n",
    "  * Similar to the temporal lobe of the brain\n",
    "* RNNs are like short-term memory\n",
    "  * Similar to the frontal lobe of the brain\n",
    "* Can be thought of as a ANN squashed together (or looking from another dimension) and inverted with a temporal loop with in the hidden layer\n",
    "  * Temporal loop meaning it feeds data back into itself in addition to the output\n",
    "  * Each blue circle representing a whole layer of neurons <img src=\"img/41_blog_image_10.png\">\n",
    "* Types of RNNs\n",
    "  * 1:Many - One input and many outputs\n",
    "    * e.g. An image input with a descriptive output of the image. \"black and white dog jumps over bar\"\n",
    "  * Many:1 - Many inputs and one output\n",
    "    * e.g. Sentiment analysis. \"Thanks for a great party!\" - positive score: 86%\n",
    "  * Many:Many - Many inputs to many outputs\n",
    "    * e.g. Google translator to maintain gender specific information\n",
    "    * e.g. Movie subtitles - need context to understand plot\n",
    "* `Vanishing gradient problem`\n",
    "  * As weights are updated backward through the network if the value (error) being applied to the weights (Wrec) is too small then the weights get too small to be relevant (vanishign) or if too big then the weights get too large (exploding) and invalidates the entire network\n",
    "* `Long short-term memory` (LSTM)\n",
    "  *  Solution to vanishing gradient problem <img src=\"img/41_blog_image_22.png\" >\n",
    "  *  pointwise operations are like valves (X) controlled by neural network layers\n",
    "     *  forget valve\n",
    "     *  memory valve\n",
    "     *  output valve\n",
    "  *  neural network layer\n",
    "     *  sigmoid\n",
    "     *  tanh\n",
    "  *  Variations\n",
    "     *  Peep-hole - adding additional information to neural network layers (sigmoid)\n",
    "     *  Combine decisioning between valves\n",
    "     *  Gated recurring units (GRUs)\n",
    "  *  Reading\n",
    "     *  [LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf) by Klaus Greff et al. (2015) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A stacked LSTM model with Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, abspath, join, curdir\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[325.25],\n",
       "        [331.27],\n",
       "        [329.83],\n",
       "        ...,\n",
       "        [793.7 ],\n",
       "        [783.33],\n",
       "        [782.75]]),\n",
       " (1258, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the training set\n",
    "datapath = join(dirname(dirname(abspath(curdir))), \"data\", \"raw\")\n",
    "\n",
    "dataset_train = pd.read_csv(join(datapath, \"googl_stock_prices_2012-2016.csv\"))\n",
    "train_set = dataset_train.iloc[:, 1:2].values\n",
    "train_set, train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.08581368],\n",
       "        [0.09701243],\n",
       "        [0.09433366],\n",
       "        ...,\n",
       "        [0.95725128],\n",
       "        [0.93796041],\n",
       "        [0.93688146]]),\n",
       " 1258)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply recommended normalization scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "train_set_scaled = sc.fit_transform(train_set) # Fit with data min/max then transforms\n",
    "train_set_scaled, train_set_scaled.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data structure with 60 timesteps (past info) and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, train_set_scaled.size):\n",
    "    X_train.append(train_set_scaled[i-60:i, 0])\n",
    "    y_train.append(train_set_scaled[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368, 0.09701243, 0.09433366, ..., 0.07846566, 0.08034452,\n",
       "        0.08497656],\n",
       "       [0.09701243, 0.09433366, 0.09156187, ..., 0.08034452, 0.08497656,\n",
       "        0.08627874],\n",
       "       [0.09433366, 0.09156187, 0.07984225, ..., 0.08497656, 0.08627874,\n",
       "        0.08471612],\n",
       "       ...,\n",
       "       [0.92106928, 0.92438053, 0.93048218, ..., 0.95475854, 0.95204256,\n",
       "        0.95163331],\n",
       "       [0.92438053, 0.93048218, 0.9299055 , ..., 0.95204256, 0.95163331,\n",
       "        0.95725128],\n",
       "       [0.93048218, 0.9299055 , 0.93113327, ..., 0.95163331, 0.95725128,\n",
       "        0.93796041]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08627874, 0.08471612, 0.07454052, ..., 0.95725128, 0.93796041,\n",
       "       0.93688146])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.08581368],\n",
       "         [0.09701243],\n",
       "         [0.09433366],\n",
       "         ...,\n",
       "         [0.07846566],\n",
       "         [0.08034452],\n",
       "         [0.08497656]],\n",
       " \n",
       "        [[0.09701243],\n",
       "         [0.09433366],\n",
       "         [0.09156187],\n",
       "         ...,\n",
       "         [0.08034452],\n",
       "         [0.08497656],\n",
       "         [0.08627874]],\n",
       " \n",
       "        [[0.09433366],\n",
       "         [0.09156187],\n",
       "         [0.07984225],\n",
       "         ...,\n",
       "         [0.08497656],\n",
       "         [0.08627874],\n",
       "         [0.08471612]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.92106928],\n",
       "         [0.92438053],\n",
       "         [0.93048218],\n",
       "         ...,\n",
       "         [0.95475854],\n",
       "         [0.95204256],\n",
       "         [0.95163331]],\n",
       " \n",
       "        [[0.92438053],\n",
       "         [0.93048218],\n",
       "         [0.9299055 ],\n",
       "         ...,\n",
       "         [0.95204256],\n",
       "         [0.95163331],\n",
       "         [0.95725128]],\n",
       " \n",
       "        [[0.93048218],\n",
       "         [0.9299055 ],\n",
       "         [0.93113327],\n",
       "         ...,\n",
       "         [0.95163331],\n",
       "         [0.95725128],\n",
       "         [0.93796041]]]),\n",
       " (1198, 60, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping to add new dimension to train data, the new dimension can be any \n",
    "# number of additional indicators but for now all we're using is the open price\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_train, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 22:35:57.664897: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RNN\n",
    "rnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first LSTM layer with dropout regularization to prevent overfitting\n",
    "\n",
    "# units = # of neurons, need high dimensionality to capture upward/downward trends\n",
    "# return_sequences = True when there will be multiple LSTM layers\n",
    "rnn.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "rnn.add(Dropout(0.2)) # ignore 20% of neurons during each training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add second LSTM/Dropout layers\n",
    "rnn.add(LSTM(units=50, return_sequences=True))\n",
    "rnn.add(Dropout(0.2))\n",
    "\n",
    "# Add third LSTM/Dropout layers\n",
    "rnn.add(LSTM(units=50, return_sequences=True))\n",
    "rnn.add(Dropout(0.2))\n",
    "\n",
    "# Add fourth LSTM/Dropout layers\n",
    "rnn.add(LSTM(units=50))\n",
    "rnn.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the output layer\n",
    "rnn.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the RNN\n",
    "rnn.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 8s 52ms/step - loss: 0.0412\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0073\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0051\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0051\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0048\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0049\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0047\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0048\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0044\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0049\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0041\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0041\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0040\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0044\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0037\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0037\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0038\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0037\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0032\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0038\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0034\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0035\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0032\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0030\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0031\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0030\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0028\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0031\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0033\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0033\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0029\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0031\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0032\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0028\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0028\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0032\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0030\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0029\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0026\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 2s 61ms/step - loss: 0.0025\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0026\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0025\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0028\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0026\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0025\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0025\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.0024\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.0023\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0023\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0025\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0025\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0023\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0024\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 2s 56ms/step - loss: 0.0024\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0020\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0021\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 0.0020\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 2s 61ms/step - loss: 0.0021\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 2s 60ms/step - loss: 0.0021\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 0.0019\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 2s 59ms/step - loss: 0.0021\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 2s 60ms/step - loss: 0.0019\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 2s 59ms/step - loss: 0.0021\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 2s 58ms/step - loss: 0.0022\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 2s 58ms/step - loss: 0.0019\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 2s 58ms/step - loss: 0.0021\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 0.0019\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 2s 60ms/step - loss: 0.0018\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 2s 58ms/step - loss: 0.0019\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 2s 59ms/step - loss: 0.0020\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 2s 56ms/step - loss: 0.0019\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 2s 59ms/step - loss: 0.0019\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 0.0019\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0019\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.0019\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.0016\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0017\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0018\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0016\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0015\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0017\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0017\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0017\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0017\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0017\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0018\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0016\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0015\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0015\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0017\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0018\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0014\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0014\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0012\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0015\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.0014\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0014\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.0013\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1aec4c490>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the training set\n",
    "rnn.fit(X_train, y_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
